{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10152044,"sourceType":"datasetVersion","datasetId":6267527},{"sourceId":10158271,"sourceType":"datasetVersion","datasetId":6272217},{"sourceId":10172839,"sourceType":"datasetVersion","datasetId":6283020},{"sourceId":104625,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72253,"modelId":76277}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SQuAD-v1","metadata":{}},{"cell_type":"code","source":"import torch\nimport gc\n\ntorch.cuda.empty_cache()  # Free unused GPU memory\ngc.collect()  # Garbage collect to free RAM\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:36:29.581845Z","iopub.execute_input":"2024-12-12T12:36:29.582155Z","iopub.status.idle":"2024-12-12T12:36:29.586005Z","shell.execute_reply.started":"2024-12-12T12:36:29.582129Z","shell.execute_reply":"2024-12-12T12:36:29.585014Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:36:31.662893Z","iopub.execute_input":"2024-12-12T12:36:31.663206Z","iopub.status.idle":"2024-12-12T12:36:32.770219Z","shell.execute_reply.started":"2024-12-12T12:36:31.663180Z","shell.execute_reply":"2024-12-12T12:36:32.769369Z"}},"outputs":[{"name":"stdout","text":"Thu Dec 12 12:36:32 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   54C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   49C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Load the dataset\nfile_path = \"/kaggle/input/eng-benchmark/SQuAD-v1.1.csv\"  # Path to the dataset\ndata = pd.read_csv(file_path)\nbenchmark_data = data.head(50)  # Use the first 10 rows for testing\n\n# Load the model and tokenizer\nmodel_name = \"/kaggle/input/gemma-2/transformers/gemma-2-2b/2\"  # Replace with your model path if needed\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(\"Loading the model and tokenizer...\")\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprint(\"Model and tokenizer loaded successfully.\")\n\n# Evaluation function\ndef evaluate_model(row):\n    question = row[\"question\"]\n    context = row[\"context\"]\n    \n    # Explicit prompt to enforce answering from the context\n    input_text = (\n        f\"Read the following context and answer the question strictly based on it, give me only one answer, do not give me A, B, C and D.\\n\\n\"\n        f\"Context: {context}\\n\"\n        f\"Question: {question}\\n\"\n        f\"Answer:\"\n    )\n\n    # Tokenize input\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n\n    # Generate output with controlled length\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=50)\n\n    # Decode and extract the answer\n    generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Post-process: Remove prompt from generated text\n    if \"Answer:\" in generated_answer:\n        generated_answer = generated_answer.split(\"Answer:\")[-1].strip()\n\n    return generated_answer\n\n# Evaluate the model\nresults = []\nprint(\"Evaluating model on the dataset...\")\nfor _, row in benchmark_data.iterrows():\n    generated_answer = evaluate_model(row)\n    results.append({\n        \"question\": row[\"question\"],\n        \"context\": row[\"context\"],\n        \"expected_answer\": row[\"answer\"],\n        \"generated_answer\": generated_answer\n    })\n\n# Save results to a CSV file\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"squad_benchmark_results4.csv\", index=False)\n\nprint(\"Evaluation completed. Results saved to 'squad_benchmark_results.csv'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:36:49.037275Z","iopub.execute_input":"2024-12-12T12:36:49.038153Z","iopub.status.idle":"2024-12-12T12:40:36.941336Z","shell.execute_reply.started":"2024-12-12T12:36:49.038118Z","shell.execute_reply":"2024-12-12T12:40:36.940418Z"}},"outputs":[{"name":"stdout","text":"Loading the model and tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4e6fbb3a41a4538a763fccd18da96b1"}},"metadata":{}},{"name":"stdout","text":"Model and tokenizer loaded successfully.\nEvaluating model on the dataset...\nEvaluation completed. Results saved to 'squad_benchmark_results.csv'.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# ROUGE & BLUE SCORE","metadata":{}},{"cell_type":"code","source":"!pip install rouge_score\n!pip install bert_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# Load original and generated data\noriginal_data_path = \"/kaggle/input/eng-benchmark/SQuAD-v1.1.csv\"  # Original file path\nresults_data_path = \"/kaggle/working/squad_benchmark_results.csv\"  # Your results file\n\n# Load the original and generated data\noriginal_data = pd.read_csv(original_data_path).head(500)\ngenerated_data = pd.read_csv(results_data_path)\n\n# Extract relevant columns\nexpected_answers = original_data['answer'].tolist()\ngenerated_answers = generated_data['generated_answer'].tolist()\n\n# Initialize ROUGE scorer\nrouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\n# Calculate ROUGE scores\nrouge_scores = []\nfor expected, generated in zip(expected_answers, generated_answers):\n    scores = rouge_scorer_obj.score(str(expected), str(generated))\n    rouge_scores.append(scores)\n\n# Calculate average ROUGE scores\navg_rouge1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)\navg_rouge2 = sum(score['rouge2'].fmeasure for score in rouge_scores) / len(rouge_scores)\navg_rougeL = sum(score['rougeL'].fmeasure for score in rouge_scores) / len(rouge_scores)\n\nprint(f\"Average ROUGE-1: {avg_rouge1:.2f}\")\nprint(f\"Average ROUGE-2: {avg_rouge2:.2f}\")\nprint(f\"Average ROUGE-L: {avg_rougeL:.2f}\")\n\n\n# BLEU Score Calculation\nsmooth_func = SmoothingFunction().method1  # Smoothing to handle short sentences\n\nbleu_scores = []\nfor expected, generated in zip(expected_answers, generated_answers):\n    reference = [str(expected).split()]  # BLEU expects a list of reference tokens\n    hypothesis = str(generated).split()  # Hypothesis (generated answer)\n    score = sentence_bleu(reference, hypothesis, smoothing_function=smooth_func)\n    bleu_scores.append(score)\n\naverage_bleu = sum(bleu_scores) / len(bleu_scores)\n\nprint(f\"Average BLEU Score: {average_bleu:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nfrom datasets import load_dataset\n\n# Load the first 50 rows of the MMLU dataset\nds = load_dataset(\"cais/mmlu\", \"all\")\ntest_data = ds[\"test\"].select(range(50))  # First 50 rows for benchmarking\n\nbenchmark_data = pd.DataFrame({\n    \"question\": test_data[\"question\"],\n    \"choices\": test_data[\"choices\"],\n    \"answer\": test_data[\"answer\"]\n})\n\n# Simulate or load context (for this example, using a placeholder context)\n# Replace this with real context if available\ncontext = \"This is a general knowledge context. Use it to answer the question accurately.\"\n\n# Load the Gemma model and tokenizer\nmodel_name = \"/kaggle/input/gemma-2/transformers/gemma-2-2b/2\"  # Replace with the correct path\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Evaluation function with context\ndef evaluate_model(row, context):\n    question = row[\"question\"]\n    choices = row[\"choices\"]\n    correct_answer_index = row[\"answer\"]\n    correct_answer = choices[correct_answer_index]\n\n    # Construct a clear input prompt with context\n    input_text = (\n        f\"Context: {context}\\n\\n\"\n        f\"Answer the following question by choosing the correct option.\\n\\n\"\n        f\"Question: {question}\\n\"\n        f\"Choices:\\n\"\n        f\"A) {choices[0]}\\n\"\n        f\"B) {choices[1]}\\n\"\n        f\"C) {choices[2]}\\n\"\n        f\"D) {choices[3]}\\n\"\n        \"Answer:\"\n    )\n\n    # Tokenize input\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n    \n    # Generate output\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=50, eos_token_id=tokenizer.eos_token_id)\n    \n    # Decode and clean the output\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n    generated_answer = None\n    \n    # Match the generated text to one of the choices\n    for choice in choices:\n        if choice in generated_text:\n            generated_answer = choice\n            break\n\n    # If no match, return the full generated text for debugging\n    return generated_answer if generated_answer else generated_text\n\n# Evaluate the model\nresults = []\nfor _, row in benchmark_data.iterrows():\n    generated_answer = evaluate_model(row, context)\n    correct_answer = row[\"choices\"][row[\"answer\"]]\n    results.append({\n        \"question\": row[\"question\"],\n        \"correct_answer\": correct_answer,\n        \"generated_answer\": generated_answer\n    })\n\n# Convert results to a DataFrame\nresults_df = pd.DataFrame(results)\n\n# Save results to a file\nresults_df.to_csv(\"gemma_mmlu_with_context_results.csv\", index=False)\nprint(\"Evaluation completed. Results saved to 'gemma_mmlu_with_context_results.csv'.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"benchmark_data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom rouge_score import rouge_scorer\nfrom datasets import load_dataset\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# Load the MMLU dataset\nds = load_dataset(\"cais/mmlu\", \"all\")\ntest_data = ds[\"test\"].select(range(50))  # First 50 rows for benchmarking\n\n# Prepare the original data DataFrame\noriginal_data = pd.DataFrame({\n    \"question\": test_data[\"question\"],\n    \"choices\": test_data[\"choices\"],\n    \"answer\": test_data[\"answer\"]\n})\n\n# Load the generated results\nresults_data_path = \"/kaggle/working/gemma_mmlu_with_context_results.csv\"  # Path to generated results\ngenerated_data = pd.read_csv(results_data_path)\n\n# Extract the correct answers\ncorrect_answers = [\n    row['choices'][row['answer']]  # Extract the correct answer using the index\n    for _, row in original_data.iterrows()\n]\n\n# Extract the generated answers\ngenerated_answers = generated_data['generated_answer'].tolist()\n\n# Initialize ROUGE scorer\nrouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\n# Calculate ROUGE scores\nrouge_scores = []\nfor correct, generated in zip(correct_answers, generated_answers):\n    scores = rouge_scorer_obj.score(str(correct), str(generated))\n    rouge_scores.append(scores)\n\n# Calculate average ROUGE scores\navg_rouge1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)\navg_rouge2 = sum(score['rouge2'].fmeasure for score in rouge_scores) / len(rouge_scores)\navg_rougeL = sum(score['rougeL'].fmeasure for score in rouge_scores) / len(rouge_scores)\n\nprint(f\"Average ROUGE-1: {avg_rouge1:.2f}\")\nprint(f\"Average ROUGE-2: {avg_rouge2:.2f}\")\nprint(f\"Average ROUGE-L: {avg_rougeL:.2f}\")\n\n\n# BLEU Score Calculation\nsmooth_func = SmoothingFunction().method1  # Use smoothing to handle short sentences\n\nbleu_scores = []\nfor correct, generated in zip(correct_answers, generated_answers):\n    reference = [str(correct).split()]  # BLEU expects tokenized reference as a list\n    hypothesis = str(generated).split()  # Tokenized hypothesis (generated answer)\n    score = sentence_bleu(reference, hypothesis, smoothing_function=smooth_func)\n    bleu_scores.append(score)\n\naverage_bleu = sum(bleu_scores) / len(bleu_scores)\n\n# Print BLEU score\nprint(f\"Average BLEU Score: {average_bleu:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LLM judge CHATgpt","metadata":{}},{"cell_type":"code","source":"!pip install openai pandas tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:49:53.193671Z","iopub.execute_input":"2024-12-12T12:49:53.194019Z","iopub.status.idle":"2024-12-12T12:50:03.329912Z","shell.execute_reply.started":"2024-12-12T12:49:53.193990Z","shell.execute_reply":"2024-12-12T12:50:03.328820Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting openai\n  Downloading openai-1.57.2-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.4.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.27.0)\nCollecting jiter<1,>=0.4.0 (from openai)\n  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.10.1)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.10/site-packages (from openai) (4.12.2)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nDownloading openai-1.57.2-py3-none-any.whl (389 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.9/389.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: jiter, openai\nSuccessfully installed jiter-0.8.2 openai-1.57.2\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install --upgrade openai\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:50:03.332242Z","iopub.execute_input":"2024-12-12T12:50:03.333168Z","iopub.status.idle":"2024-12-12T12:50:11.527857Z","shell.execute_reply.started":"2024-12-12T12:50:03.333124Z","shell.execute_reply":"2024-12-12T12:50:11.526706Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: openai in /opt/conda/lib/python3.10/site-packages (1.57.2)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.4.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.27.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.8.2)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.10.1)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.4)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.10/site-packages (from openai) (4.12.2)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import openai\nprint(openai.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Relevance","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom openai import OpenAI\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# OpenAI API setup\nclient = OpenAI(api_key = \"\")  # Replace with your key\n\n# Load Dataset\nfile_path = \"/kaggle/working/gemma_mmlu_with_context_results.csv\"  # Replace with your dataset path\ndf = pd.read_csv(file_path)\n\n# Function to generate a question from a given answer using ChatGPT\ndef generate_question_from_answer(answer):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4\",  # Use GPT model available to you\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant generating questions.\"},\n                {\"role\": \"user\", \"content\": f\"Generate a question for which this could be an answer: {answer}\"}\n            ],\n            max_tokens=50,\n            temperature=0.0\n        )\n        return response.choices[0].message.content.strip()\n    except Exception as e:\n        print(f\"Error generating question: {e}\")\n        return None\n\n# Function to generate embeddings for a text\ndef get_embeddings(text):\n    try:\n        response = client.embeddings.create(\n            model=\"text-embedding-3-small\",  # Embedding model\n            input=text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        print(f\"Error generating embeddings: {e}\")\n        return None\n\n# Calculate Relevance\ndef calculate_relevance(question, generated_question):\n    try:\n        # Generate embeddings\n        question_emb = get_embeddings(question)\n        generated_question_emb = get_embeddings(generated_question)\n        \n        if question_emb and generated_question_emb:\n            # Calculate cosine similarity\n            similarity = cosine_similarity(\n                [question_emb],\n                [generated_question_emb]\n            )[0][0]\n            return similarity\n        return None\n    except Exception as e:\n        print(f\"Error calculating relevance: {e}\")\n        return None\n\n# Process Dataset\ntqdm.pandas()\ndf[\"generated_question\"] = df[\"generated_answer\"].progress_apply(generate_question_from_answer)\ndf[\"relevance_score\"] = df.progress_apply(\n    lambda row: calculate_relevance(row[\"question\"], row[\"generated_question\"]), axis=1\n)\n\n# Save results\noutput_file = \"relevance_results2.xlsx\"\ndf.to_excel(output_file, index=False)\nprint(f\"Relevance scores saved to {output_file}\")\nprint(df[[\"question\", \"generated_answer\", \"generated_question\", \"relevance_score\"]])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Faithfulness","metadata":{}},{"cell_type":"code","source":"import os\nfrom openai import OpenAI\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport re\n\n# OpenAI API setup\nclient = OpenAI(api_key = \"\")  # Replace with your key\n\n# Load Dataset\nfile_path = \"/kaggle/working/squad_benchmark_results4.csv\"  # Replace with your dataset path\ndf = pd.read_csv(file_path)\n\n# System prompt for claim extraction\nCLAIM_EXTRACTION_PROMPT = \"\"\"\nExtract all factual claims made in the following text. Present each claim as a concise statement:\n\nText: {answer}\n\nClaims:\n\"\"\"\n\n# System prompt for claim verification\nCLAIM_VERIFICATION_PROMPT = \"\"\"\nYou will be given a factual claim and some context. Your task is to determine whether the claim matches the context.\n\nFor each claim:\n- If the claim agrees with the context, respond \"Yes\".\n- If the claim cannot be verified from the context, respond \"Idk\".\n- If the claim contradicts the context, respond \"No\".\n\nClaim: {claim}\nContext: {context}\n\nAnswer (Yes, Idk, or No):\n\"\"\"\n\n# Function to extract claims from an answer\ndef extract_claims(answer):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert at extracting factual claims.\"},\n                {\"role\": \"user\", \"content\": CLAIM_EXTRACTION_PROMPT.format(answer=answer)}\n            ],\n            max_tokens=200,\n            temperature=0.0\n        )\n        # Split extracted claims into a list\n        claims = [claim.strip() for claim in response.choices[0].message.content.strip().split(\"\\n\") if claim.strip()]\n        return claims\n    except Exception as e:\n        print(f\"Error extracting claims: {e}\")\n        return []\n\n# Function to verify claims against context\ndef verify_claim(claim, context):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a fact-checking assistant.\"},\n                {\"role\": \"user\", \"content\": CLAIM_VERIFICATION_PROMPT.format(claim=claim, context=context)}\n            ],\n            max_tokens=10,\n            temperature=0.0\n        )\n        return response.choices[0].message.content.strip()\n    except Exception as e:\n        print(f\"Error verifying claim: {e}\")\n        return \"Idk\"\n\n# Function to calculate faithfulness score\ndef calculate_faithfulness(answer, context):\n    try:\n        # Step 1: Extract claims\n        claims = extract_claims(answer)\n        if not claims:\n            return 0  # No claims found, faithfulness is 0\n\n        # Step 2: Verify claims\n        valid_claims = 0\n        for claim in claims:\n            result = verify_claim(claim, context)\n            if result in [\"Yes\", \"Idk\"]:\n                valid_claims += 1\n\n        # Step 3: Calculate faithfulness score\n        faithfulness_score = valid_claims / len(claims)\n        return faithfulness_score\n    except Exception as e:\n        print(f\"Error calculating faithfulness: {e}\")\n        return 0\n\n# Process Dataset\ntqdm.pandas()\ndf[\"faithfulness_score\"] = df.progress_apply(\n    lambda row: calculate_faithfulness(row[\"generated_answer\"], row[\"context\"]), axis=1\n)\n\n# Save results\noutput_file = \"squad_faithfulness_results4.xlsx\"\ndf.to_excel(output_file, index=False)\nprint(f\"Faithfulness scores saved to {output_file}\")\nprint(df[[\"question\", \"generated_answer\", \"faithfulness_score\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:50:11.530021Z","iopub.execute_input":"2024-12-12T12:50:11.530316Z","iopub.status.idle":"2024-12-12T12:52:07.248203Z","shell.execute_reply.started":"2024-12-12T12:50:11.530289Z","shell.execute_reply":"2024-12-12T12:52:07.247284Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b37b63114f44fe9b16ef5eb0d66a98c"}},"metadata":{}},{"name":"stdout","text":"Faithfulness scores saved to squad_faithfulness_results4.xlsx\n                                             question  \\\n0   To whom did the Virgin Mary allegedly appear i...   \n1   What is in front of the Notre Dame Main Building?   \n2   The Basilica of the Sacred heart at Notre Dame...   \n3                   What is the Grotto at Notre Dame?   \n4   What sits on top of the Main Building at Notre...   \n5   When did the Scholastic Magazine of Notre dame...   \n6    How often is Notre Dame's the Juggler published?   \n7   What is the daily student paper at Notre Dame ...   \n8   How many student news papers are found at Notr...   \n9   In what year did the student paper Common Sens...   \n10  Where is the headquarters of the Congregation ...   \n11  What is the primary seminary of the Congregati...   \n12        What is the oldest structure at Notre Dame?   \n13  What individuals live at Fatima House at Notre...   \n14         Which prize did Frederick Buechner create?   \n15  How many BS level degrees are offered in the C...   \n16  In what year was the College of Engineering at...   \n17  Before the creation of the College of Engineer...   \n18  How many departments are within the Stinson-Re...   \n19  The College of Science began to offer civil en...   \n20  What entity provides help with the management ...   \n21  How many colleges for undergraduates are at No...   \n22  What was created at Notre Dame in 1962 to assi...   \n23  Which organization declared the First Year of ...   \n24  The granting of Doctorate degrees first occurr...   \n25                  What type of degree is an M.Div.?   \n26  Which program at Notre Dame offers a Master of...   \n27  In what year was a Master of Arts course first...   \n28  Which department at Notre Dame is the only one...   \n29  What institute at Notre Dame studies  the reas...   \n30  What is the title of Notre Dame's Theodore Hes...   \n31  In what year was the Joan B. Kroc Institute fo...   \n32                  To whom was John B. Kroc married?   \n33                     What company did Ray Kroc own?   \n34  How many stories tall is the main library at N...   \n35  What is the name of the main library at Notre ...   \n36  In what year was the Theodore M. Hesburgh Libr...   \n37  Which artist created the mural on the Theodore...   \n38  What is a common name to reference the mural c...   \n39  How many incoming students did Notre Dame admi...   \n40  What percentage of students were admitted to N...   \n41  Where does Notre Dame rank in terms of academi...   \n42  What percentage of students at Notre Dame part...   \n43  How many miles does the average student at Not...   \n44  Where did U.S. News & World Report rank Notre ...   \n45  Forbes.com placed Notre Dame at what position ...   \n46  The undergrad school at the Mendoza College of...   \n47  In 2014 what entity named Notre Dame 10th best...   \n48  What percentage of Notre Dame students decide ...   \n49  What person was the Director of the Science Mu...   \n\n                                     generated_answer  faithfulness_score  \n0   A. Saint Bernadette Soubirous\\nB. Saint Bernad...                1.00  \n1   A. A statue of the Virgin Mary\\nB. A statue of...                0.75  \n2   A. The Grotto\\nB. The Main Building\\nC. The Go...                1.00  \n3   A. A place of prayer and reflection\\nB. A repl...                1.00  \n4   A. A statue of the Virgin Mary\\nB. A statue of...                0.75  \n5                  A. 1876\\nB. 1886\\nC. 1896\\nD. 1906                0.25  \n6   A. Twice a year\\nB. Twice a month\\nC. Once a y...                1.00  \n7   A. The Observer\\nB. The Juggler\\nC. The Dome\\n...                1.00  \n8                              A. 2\\nB. 3\\nC. 4\\nD. 5                1.00  \n9                  A. 1987\\nB. 1997\\nC. 2003\\nD. 2008                0.75  \n10  A. Rome\\nB. Notre Dame\\nC. Chicago\\nD. Rome an...                1.00  \n11  A. Moreau Seminary\\nB. Holy Cross House\\nC. Co...                1.00  \n12  A. Old College\\nB. Moreau Seminary\\nC. Columba...                1.00  \n13  A. Retired priests and brothers\\nB. Undergradu...                1.00  \n14  A. Buechner Prize for Preaching\\nB. Buechner P...                0.25  \n15                             A. 5\\nB. 6\\nC. 7\\nD. 8                1.00  \n16                 A. 1920\\nB. 1870\\nC. 1950\\nD. 1960                0.50  \n17  A. College of Arts and Letters\\nB. College of ...                1.00  \n18                             A. 1\\nB. 2\\nC. 3\\nD. 4                1.00  \n19                A. 1870s\\nB. 1920\\nC. 1940\\nD. 1960                1.00  \n20  A. The First Year of Studies program\\nB. The L...                0.75  \n21                             A. 5\\nB. 4\\nC. 3\\nD. 2                1.00  \n22  A. The First Year of Studies program\\nB. The L...                1.00  \n23  A. U.S. News & World Report\\nB. Notre Dame\\nC....                1.00  \n24                 A. 1854\\nB. 1855\\nC. 1924\\nD. 1954                0.75  \n25  A. Master of Arts\\nB. Master of Science\\nC. Ma...                1.00  \n26  A. College of Arts and Letters\\nB. College of ...                1.00  \n27                 A. 1854\\nB. 1855\\nC. 1856\\nD. 1857                0.50  \n28  A. College of Arts and Letters\\nB. College of ...                1.00  \n29  A. Joan B. Kroc Institute for International Pe...                0.50  \n30  A. President\\nB. Vice President\\nC. Chancellor...                0.75  \n31                 A. 1986\\nB. 1987\\nC. 1988\\nD. 1989                0.25  \n32  A. Joan B. Kroc\\nB. Joan B. Kroc's husband\\nC....                0.40  \n33  A. McDonald's\\nB. Burger King\\nC. KFC\\nD. None...                0.50  \n34                         A. 14\\nB. 13\\nC. 12\\nD. 11                1.00  \n35  A. Hesburgh Library\\nB. Hesburgh Library\\nC. H...                0.00  \n36                 A. 1963\\nB. 1962\\nC. 1961\\nD. 1960                0.25  \n37  A. Millard Sheets\\nB. John Paul II\\nC. John Pa...                0.25  \n38  A. Touchdown Jesus\\nB. Word of Life\\nC. Notre ...                1.00  \n39  A. 3,577\\nB. 18,156\\nC. 19,700\\nD. 3,577 and 1...                0.75  \n40             A. 19.7%\\nB. 39.1%\\nC. 58.8%\\nD. 74.8%                0.50  \n41                 A. 10th\\nB. 11th\\nC. 12th\\nD. 13th                0.00  \n42               A. 39.1%\\nB. 19.7%\\nC. 35.7%\\nD. 25%                0.50  \n43  A. 750 miles\\nB. 1,000 miles\\nC. 1,500 miles\\n...                0.50  \n44                 A. 18th\\nB. 10th\\nC. 13th\\nD. 22nd                1.00  \n45                     A. 1st\\nB. 2nd\\nC. 3rd\\nD. 4th                1.00  \n46  1. 1st overall\\n2. 20th overall\\n3. 12th overa...                0.25  \n47  A. USA Today\\nB. Forbes.com\\nC. U.S. News & Wo...                1.00  \n48             A. 57.6%\\nB. 57.6%\\nC. 57.6%\\nD. 57.6%                1.00  \n49  A. Father Joseph Carrier\\nB. Father John Augus...                0.75  \n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}