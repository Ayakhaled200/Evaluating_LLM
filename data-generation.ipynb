{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Web Scraping","metadata":{}},{"cell_type":"code","source":"!pip install requests beautifulsoup4 pandas\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Scraping the Articles","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# رابط ويكيبيديا العربي\nURL = \"https://ar.wikipedia.org/wiki/\"\n\n# أسماء المقالات المراد جلبها\narticles = [\"الخيمياء والكيمياء في عصر الحضارة الإسلامية\",\"تدمير البيئة\"\n            ,\"محرك نفاث\",\"السياحة في السعودية\",\"خط حديد الحجاز\", \"تقطيع (شبكات)\", \"ويندوز فيستا\"]\n\ndef scrape_wikipedia(article):\n    # إرسال طلب للمقال\n    response = requests.get(URL + article)\n    if response.status_code == 200:\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        \n        # استخراج النصوص من الفقرات <p>\n        paragraphs = soup.find_all('p')\n        text = [p.get_text() for p in paragraphs]\n        \n        # دمج النصوص معًا\n        return \" \".join(text)\n    else:\n        print(f\"Failed to fetch {article}\")\n        return \"\"\n\n# تخزين النصوص المُستخرجة\ndata = []\nfor article in articles:\n    print(f\"Scraping: {article}\")\n    text = scrape_wikipedia(article)\n    data.append({\"Article\": article, \"Content\": text})\n\n# حفظ البيانات في DataFrame\ndf = pd.DataFrame(data)\nprint(df.head())\n\n# حفظ النصوص المُستخرجة في ملف CSV\ndf.to_csv(\"arabic_wikipedia_articles.csv\", index=False, encoding='utf-8-sig')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install openai pandas tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade openai\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom openai import OpenAI\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# OpenAI API setup\nclient = OpenAI(api_key = \"\")  # Replace with your key\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport openai\n\n# Load the CSV file\nfile_path = \"/kaggle/working/arabic_wikipedia_articles.csv\"  # Replace with your file path\ndf = pd.read_csv(file_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_question_answer(content, article_title, model=\"gpt-4\", max_tokens=200):\n    \"\"\"\n    Generate a question and answer in Arabic from the content of an article.\n    \"\"\"\n    prompt = (\n        f\"قم بإنشاء سؤال وإجابة باللغة العربية استنادًا إلى النص التالي من المقال بعنوان '{article_title}':\\n\\n\"\n        f\"{content}\\n\\n\"\n        \"يرجى تقديم النتيجة بهذا التنسيق:\\n\"\n        \"السؤال: <السؤال المولد>\\n\"\n        \"الإجابة: <الإجابة المولدة>\"\n    )\n\n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=max_tokens,\n            temperature=0.5\n        )\n        # Extract the output\n        output = response.choices[0].message.content.strip()\n        question, answer = output.split(\"\\n\")[:2]  # Split into question and answer\n        return question.replace(\"السؤال: \", \"\"), answer.replace(\"الإجابة: \", \"\")\n    except Exception as e:\n        print(f\"Error generating Q&A: {e}\")\n        return None, None\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n# Function to chunk long content into smaller parts\ndef chunk_text(text, chunk_size=1500):\n    \"\"\"\n    Splits a text into smaller chunks of a specified size.\n\n    Parameters:\n        text (str): The input text to split.\n        chunk_size (int): The maximum size of each chunk in characters.\n\n    Returns:\n        list: A list of text chunks.\n    \"\"\"\n    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n\n# Function to generate questions and answers\ndef generate_question_answer(content, article_title, model=\"gpt-4\", max_tokens=200):\n    \"\"\"\n    Generate a question and answer in Arabic from the content of an article.\n    \"\"\"\n    prompt = (\n        f\"قم بإنشاء سؤال وإجابة باللغة العربية استنادًا إلى النص التالي من المقال بعنوان '{article_title}':\\n\\n\"\n        f\"{content}\\n\\n\"\n        \"يرجى تقديم النتيجة بهذا التنسيق:\\n\"\n        \"السؤال: <السؤال المولد>\\n\"\n        \"الإجابة: <الإجابة المولدة>\"\n    )\n\n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=max_tokens,\n            temperature=0.5\n        )\n        # Extract the output\n        output = response.choices[0].message.content.strip()\n        question, answer = output.split(\"\\n\")[:2]  # Split into question and answer\n        return question.replace(\"السؤال: \", \"\"), answer.replace(\"الإجابة: \", \"\")\n    except Exception as e:\n        print(f\"Error generating Q&A: {e}\")\n        return None, None\n\n# Load the input data\ninput_file = \"/kaggle/working/arabic_wikipedia_articles.csv\"  # Path to your input file\noutput_file = \"generated_questions_answers.csv\"\n\ntry:\n    # Read the input CSV file (expecting 'article' and 'content' columns)\n    df = pd.read_csv(input_file)\n\n    # Initialize a list to store results\n    results = []\n\n    # Process each article\n    print(\"Generating questions and answers...\")\n    for _, row in df.iterrows():\n        article_title = row['Article']  # Article title\n        content = row['Content']       # Article content\n\n        # Split the content into smaller chunks\n        chunks = chunk_text(content, chunk_size=1500)\n\n        # Generate Q&A for each chunk\n        for idx, chunk in enumerate(chunks):\n            print(f\"Processing chunk {idx + 1} for article: {article_title}\")\n            question, answer = generate_question_answer(chunk, article_title)\n\n            # Append results to the list\n            if question and answer:\n                results.append({\n                    \"article\": article_title,\n                    \"chunk_number\": idx + 1,\n                    \"chunk_content\": chunk,\n                    \"question\": question,\n                    \"answer\": answer\n                })\n\n    # Convert results to a DataFrame\n    results_df = pd.DataFrame(results)\n\n    # Save the generated Q&A to a new CSV file\n    results_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n\n    print(f\"Questions and answers saved to: {output_file}\")\nexcept Exception as e:\n    print(f\"Error processing the file: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}